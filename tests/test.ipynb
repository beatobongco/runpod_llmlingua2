{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\joshl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\joshl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (1.26.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\joshl\\appdata\\roaming\\python\\python311\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\joshl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\joshl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\joshl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joshl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\joshl\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk->rouge_score) (0.4.3)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 41.0/133.7 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 61.4/133.7 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  133.1/133.7 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 133.7/133.7 kB 987.7 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py): started\n",
      "  Building wheel for rouge_score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24970 sha256=78b89b06dab3859b261d883b6aabb57c011069b3ffe569598e125d9819292ad6\n",
      "  Stored in directory: c:\\users\\joshl\\appdata\\local\\pip\\cache\\wheels\\1e\\19\\43\\8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: absl-py, rouge_score\n",
      "Successfully installed absl-py-2.1.0 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Instruction tuning large language models (LLMs) using machine-generated\n",
    "instruction-following data has been shown to improve zero-shot capabilities on\n",
    "new tasks, but the idea is less explored in the multimodal field. We present the\n",
    "first attempt to use language-only GPT-4 to generate multimodal language-image\n",
    "instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained\n",
    "large multimodal model that connects a vision encoder and an LLM for generalpurpose visual and language understanding. To facilitate future research on visual\n",
    "instruction following, we construct two evaluation benchmarks with diverse and\n",
    "challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors\n",
    "of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following\n",
    "dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4\n",
    "achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated\n",
    "visual instruction tuning data, our model, and code publicly available.\n",
    "1 Introduction\n",
    "Humans interact with the world through many channels such as vision and language, as each\n",
    "individual channel has a unique advantage in representing and communicating certain concepts, and\n",
    "thus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence\n",
    "is to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language\n",
    "instructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26].\n",
    "To this end, the community has witnessed an emergent interest in developing language-augmented\n",
    "foundation vision models [27, 16], with strong capabilities in open-world visual understanding\n",
    "such as classification [40, 21, 57, 54, 39], detection [29, 62, 33], segmentation [25, 63, 58] and\n",
    "captioning [50, 28], as well as visual generation and editing [42, 43, 56, 15, 44, 30]. We refer readers\n",
    "to the Computer Vision in the Wild reading list for a more up-to-date literature compilation [12]. In\n",
    "this line of work, each task is solved independently by one single large vision model, with the task\n",
    "instruction implicitly considered in the model design. Further, language is only utilized to describe\n",
    "the image content. While this allows language to play an important role in mapping visual signals to\n",
    "language semantics—a common channel for human communication, it leads to models that usually\n",
    "have a fixed interface with limited interactivity and adaptability to the user’s instructions.\n",
    "Large language models (LLM), on the other hand, have shown that language can play a wider\n",
    "role: a universal interface for a general-purpose assistant, where various task instructions can be\n",
    "explicitly represented in language and guide the end-to-end trained neural assistant to switch to the\n",
    "task of interest to solve it. For example, the recent success of ChatGPT [35] and GPT-4 [36] have\n",
    "demonstrated the power of aligned LLMs in following human instructions, and have stimulated\n",
    "tremendous interest in developing open-source LLMs. Among them, LLaMA [49] is an opensource LLM that matches the performance of GPT-3. Alpaca [48], Vicuna [9], GPT-4-LLM [38]\n",
    "37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n",
    "arXiv:2304.08485v2 [cs.CV] 11 Dec 2023\n",
    "utilize various machine-generated high-quality instruction-following samples to improve the LLM’s\n",
    "alignment ability, reporting impressive performance compared with proprietary LLMs. Importantly,\n",
    "this line of work is text-only.\n",
    "In this paper, we present visual instruction-tuning, the first attempt to extend instruction-tuning to\n",
    "the language-image multimodal space, to pave the way towards building a general-purpose visual\n",
    "assistant. In particular, our paper makes the following contributions:\n",
    "• Multimodal instruction-following data. One key challenge is the lack of vision-language\n",
    "instruction-following data. We present a data reformation perspective and pipeline to convert\n",
    "image-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4.\n",
    "• Large multimodal models. We develop a large multimodal model (LMM), by connecting the\n",
    "open-set visual encoder of CLIP [40] with the language decoder Vicuna [9], and fine-tuning\n",
    "end-to-end on our generated instructional vision-language data. Our empirical study validates\n",
    "the effectiveness of using generated data for LMM instruction-tuning, and suggests practical\n",
    "tips for building a general-purpose instruction-following visual agent. When ensembled with\n",
    "GPT-4, our approach achieves SoTA on the Science QA [34] multimodal reasoning dataset.\n",
    "• Multimodal instruction-following benchmark. We present LLaVA-Bench with two challenging\n",
    "benchmarks, with a diverse selection of paired images, instructions and detailed annotations.\n",
    "• Open-source. We release the following assets to the public: the generated multimodal instruction\n",
    "data, the codebase, the model checkpoints, and a visual chat demo.\n",
    "2 Related Work\n",
    "Multimodal Instruction-following Agents. In computer vision, existing works that build instructionfollowing agents can be broadly categorized into two classes: (i) End-to-end trained models, which\n",
    "are separately explored for each specific research topic. For example, the vision-language navigation\n",
    "task [3, 19] and Habitat [47] require the embodied AI agent to follow natural language instructions\n",
    "and take a sequence of actions to complete goals in visual environments. In the image editing domain,\n",
    "given an input image and a written instruction that tells the agent what to do, InstructPix2Pix [6]\n",
    "edits images by following the human instructions. (ii) A system that coordinates various models\n",
    "via LangChain [1] / LLMs [35], such as Visual ChatGPT [53], X-GPT [63], MM-REACT [55],\n",
    "VisProg [18], and ViperGPT [46]. While sharing the same goal in building instruction-following\n",
    "agents, we focus on developing an end-to-end trained language-vision multimodal model for multiple\n",
    "tasks.\n",
    "Instruction Tuning. In the natural language processing (NLP) community, to enable LLMs such\n",
    "as GPT-3 [7], T5 [41], PaLM [10], and OPT [60] to follow natural language instructions and\n",
    "complete real-world tasks, researchers have explored methods for LLM instruction-tuning [37, 52, 51],\n",
    "leading to instruction-tuned counterparts such as InstructGPT [37]/ChatGPT [35], FLAN-T5 [11],\n",
    "FLAN-PaLM [11], and OPT-IML [22], respectively. It turns out that this simple approach can\n",
    "effectively improve the zero- and few-shot generalization abilities of LLMs. It is thus natural\n",
    "to borrow the idea from NLP to computer vision. More broadly, the teacher-student distillation\n",
    "ideas with foundation models have been studied in other topics such as image classification [14].\n",
    "Flamingo [2] can be viewed as the GPT-3 moment in the multimodal domain, due to its strong\n",
    "performance on zero-shot task transfer and in-context-learning. Other LMMs trained on imagetext pairs include BLIP-2 [28], FROMAGe [24], and KOSMOS-1 [20]. PaLM-E [13] is an LMM\n",
    "for embodied AI. Based on the recent “best” open-source LLM LLaMA, OpenFlamingo [5] and\n",
    "LLaMA-Adapter [59] are open-source efforts that enable LLaMA to use image inputs, paving the\n",
    "way to build open-source multimodal LLMs. While these models present promising task transfer\n",
    "generalization performance, they are not explicitly tuned with vision-language instruction data, and\n",
    "their performance in multimodal tasks usually falls short compared to language-only tasks. In this\n",
    "paper, we aim to fill this gap and study its effectiveness. Finally, note that visual instruction tuning\n",
    "is different from visual prompt tuning [23]: the former aims to improve the model’s instructionfollowing abilities, while the latter aims to improve the parameter-efficiency in model adaptation.\n",
    "3 GPT-assisted Visual Instruction Data Generation\n",
    "The community has witnessed a surge in the amount of public multimodal data such as image-text\n",
    "pairs, ranging from CC [8] to LAION [45]. However, when it comes to multimodal instruction2\n",
    "Context type 1: Captions\n",
    "A group of people standing outside of a black vehicle with various luggage.\n",
    "Luggage surrounds a vehicle in an underground parking area\n",
    "People try to fit all of their luggage in an SUV.\n",
    "The sport utility vehicle is parked in the public garage, being packed for a trip\n",
    "Some people with luggage near a van that is transporting it.\n",
    "Context type 2: Boxes\n",
    "person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ...<omitted>\n",
    "Response type 1: conversation\n",
    "Question: What type of vehicle is featured in the image?\n",
    "Answer: The image features a black sport utility vehicle (SUV) ...<omitted>\n",
    "Response type 2: detailed description\n",
    "The image is an underground parking area with a black sport utility vehicle (SUV) parked. There\n",
    "are three people in the scene, with one person standing closer to the left side of the vehicle, another\n",
    "person in the middle, and the third person on the right side. They are all working together to pack\n",
    "their luggage into the SUV for a trip. ...<omitted>\n",
    "Response type 3: complex reasoning\n",
    "Question: What challenges do these people face?\n",
    "Answer: In the image, a group of people is standing outside a black SUV in a parking area,\n",
    "surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the\n",
    "challenge of fitting all their luggage into the black SUV. There are multiple suitcases and backpacks\n",
    "to be packed, which suggests that the group has a significant amount of belongings ...<omitted>\n",
    "Table 1: One example to illustrate the instruction-following data. The top block shows the contexts\n",
    "such as captions and boxes used to prompt GPT, and the bottom block shows the three types of\n",
    "responses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.\n",
    "following data, the available amount is limited, partially because the process for creating such data is\n",
    "time-consuming and less well-defined when human crowd-scouring is considered. Inspired by the\n",
    "success of recent GPT models in text-annotation tasks [17], we propose to leverage ChatGPT/GPT-4\n",
    "for multimodal instruction-following data collection, based on the widely existing image-pair data.\n",
    "For an image Xv and its associated caption Xc, it is natural to create a set of questions Xq with the\n",
    "intent to instruct the assistant to describe the image content. We prompt GPT-4 to curate such a list\n",
    "of questions (see details in Appendix). Therefore, a simple way to expand an image-text pair to its\n",
    "instruction-following version is Human : Xq Xv<STOP> Assistant : Xc<STOP>. Though cheap to\n",
    "construct, this simple expanded version lacks diversity and in-depth reasoning in both the instructions\n",
    "and responses.\n",
    "To mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both\n",
    "accept only text as input), to create instruction-following data involving visual content. Specifically,\n",
    "in order to encode an image into its visual features to prompt a text-only GPT, we use two types of\n",
    "symbolic representations: (i) Captions typically describe the visual scene from various perspectives;\n",
    "(ii) Bounding boxes usually localize the objects in the scene, and each box encodes the object concept\n",
    "and its spatial location. One example is shown in the top block of Table 14.\n",
    "This symbolic representation allows us to encode the image as an LLM-recognizable sequence. We\n",
    "use COCO images [31] and generate three types of instruction-following data. One example per type\n",
    "is shown in the bottom block of Table 14. For each type, we first manually design a few examples.\n",
    "They are the only human annotations we have during data collection, and are used as seed examples\n",
    "in in-context-learning to query GPT-4.\n",
    "• Conversation. We design a conversation between the assistant and a person asking questions\n",
    "about this photo. The answers are in a tone as if the assistant is seeing the image and answering\n",
    "the question. A diverse set of questions are asked about the visual content of the image, including\n",
    "the object types, counting the objects, object actions, object locations, relative positions between\n",
    "objects. Only questions that have definite answers are considered. Please see Appendix for the\n",
    "detailed prompt.\n",
    "• Detailed description. To include a rich and comprehensive description for an image, we create a\n",
    "list of questions with such an intent. We prompt GPT-4 then curate the list (see detailed prompts\n",
    "3\n",
    "and curation process in Appendix). For each image, we randomly sample one question from the\n",
    "list to ask GPT-4 to generate the detailed description.\n",
    "• Complex reasoning. The above two types focus on the visual content itself, based on which\n",
    "we further create in-depth reasoning questions. The answers typically require a step-by-step\n",
    "reasoning process by following rigorous logic.\n",
    "We collect 158K unique language-image instruction-following samples in total, including 58K in\n",
    "conversations, 23K in detailed description, and 77k in complex reasoning, respectively. We ablated\n",
    "the use of ChatGPT and GPT-4 in our early experiments, and found that GPT-4 consistently provides\n",
    "higher quality instruction-following data, such as spatial reasoning.\n",
    "4 Visual Instruction Tuning\n",
    "4.1 Architecture\n",
    "The primary goal is to effectively leverage the capabilities of both the pre-trained LLM and visual\n",
    "model. The network archtecture is illustrated in Figure 1. We choose Vicuna [9] as our LLM fϕ(·)\n",
    "parameterized by ϕ, as it has the best instruction following capabilities in language tasks among\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "context2 = '''In this short tutorial, we will explore how Hugging Face models can be deployed in a Docker Container and exposed as a web service endpoint.\n",
    "\n",
    "The service it exposes is a translation service from English to French and French to English.\n",
    "\n",
    "Why someone would like to do that? Other than to learn about those specific technologies, it is a very convenient way to try and test the thousands of models that exists on Hugging Face, in a clean and isolated environment that can easily be replicated, shared or deployed elsewhere than on your local computer.\n",
    "\n",
    "In this tutorial, you will learn how to use Docker to create a container with all the necessary code and artifacts to load Hugging Face models and to expose them as web service endpoints using Flask.\n",
    "\n",
    "All code and configurations used to write this blog post are available in this GitHub Repository. You simply have to clone it and to run the commands listed in this tutorial to replicate the service on your local machine.\n",
    "\n",
    "Installing Docker\n",
    "The first step is to install Docker. The easiest way is by simply installing Docker Desktop which is available on MacOS, Windows and Linux.\n",
    "\n",
    "Creating the Dockerfile\n",
    "The next step is to create a new Git repository where you will create a Dockerfile. The Dockerfile is where all instructions are written that tells Docker how to create the container.\n",
    "\n",
    "I would also strongly encourage you to install and use hadolint, which is a really good Docker linter that helps people to follow Docker best practices. There is also a plugin for VS Code if this is what you use as you development IDE.\n",
    "\n",
    "Base image and key installs\n",
    "The first thing you define in a Dockerfile is the base image to use to initialize the container. For this tutorial, we will use Ubuntu’s latest LTS:\n",
    "\n",
    "# Use Ubuntu's current LTS\n",
    "FROM ubuntu:jammy-20230804\n",
    "\n",
    "Since we are working to create a Python web service that expose the predictions of a ML model, the next step is to add they key pieces required for the Python service. Let’s make sure that you only include what is necessary to minimize the size, and complexity, of the container as much as possible:\n",
    "\n",
    "# Make sure to not install recommends and to clean the \n",
    "# install to minimize the size of the container as much as possible.\n",
    "RUN apt-get update && \\\n",
    "    apt-get install --no-install-recommends -y python3=3.10.6-1~22.04 && \\\n",
    "    apt-get install --no-install-recommends -y python3-pip=22.0.2+dfsg-1ubuntu0.3 && \\\n",
    "    apt-get install --no-install-recommends -y python3-venv=3.10.6-1~22.04 && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "This instruct Docker to install Python3, pip and venv. It also ensures that apt get cleaned of cached files, that nothing more is installed and that we define the exact version of the package we want to install. That is to ensure that we minimize the size of the container, while making sure that the container can easily be reproduced, with the exact same codebase, any time in the future.\n",
    "\n",
    "Another thing to note: we run multiple commands with a single RUN instruction by piping them together with &&. This is to minimize the number of layers created by Docker for the container, and this is a best practice to follow when creating containers. If you don’t do this and run hadolint, then you will get warning suggesting you to refactor your Dockerfile accordingly.\n",
    "\n",
    "Copy required files\n",
    "Now that the base operating system is installed, the next step is to install all the requirements of the Python project we want to deploy in the container:\n",
    "\n",
    "# Set the working directory within the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy necessary files to the container\n",
    "COPY requirements.txt .\n",
    "COPY main.py .\n",
    "COPY download_models.py .\n",
    "\n",
    "First we define the working directory with the WORKDIR instruction. From now on, every other instruction will run from that directory in the container. We copy the local files: requirements.txt, main.py and download_models.py to the working directory.\n",
    "\n",
    "Create virtual environment\n",
    "Before doing anything with those files, we are better creating a virtual environment where to install all those dependencies. Some people may wonder why we create an environment within an environment? It is further isolation between the container and the Python application to make sure that there is no possibility of dependencies clashes. This is a good best practice to adopt.\n",
    "\n",
    "# Create a virtual environment in the container\n",
    "RUN python3 -m venv .venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "ENV PATH=\"/app/.venv/bin:$PATH\"\n",
    "\n",
    "Install application requirements\n",
    "Once the virtual environment is created and activated in the container, the next step is to install all the required dependencies in that new environment:\n",
    "\n",
    "    # Install Python dependencies from the requirements file\n",
    "RUN pip install --no-cache-dir -r requirements.txt && \\\n",
    "    # Get the models from Hugging Face to bake into the container\n",
    "    python3 download_models.py\n",
    "\n",
    "It runs pip install to install all the dependencies listed in requirements.txt. The dependencies are:\n",
    "\n",
    "transformers==4.30.2\n",
    "flask==2.3.3\n",
    "torch==2.0.1\n",
    "sacremoses==0.0.53\n",
    "sentencepiece==0.1.99\n",
    "\n",
    "Just like the Ubuntu package version, we should (have to!) pin (specify) the exact version of each dependency. This is the best way to ensure that we can reproduce this environment any time in the future and to prevent unexpected crashes because code changed in some downstream dependencies that causes issues with the code.\n",
    "\n",
    "Downloading all models in the container\n",
    "As you can see in the previous RUN command, the next step is to download all models and tokenizers in the working directory such that we bake the model’s artifacts directly in the container. That will ensures that we minimize the time it takes to initialize a container. We spend the time to download all those artifacts at build time instead of run time. The downside is that the containers will be much bigger depending on the models that are required.\n",
    "\n",
    "The download_models.py file is a utility file used to download the Hugging Face models used by the service directly into the container. The code simply download the models and tokenizer files from Hugging Face and save them locally (in the working directory of the container):\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "\n",
    "def download_model(model_path, model_name):\n",
    "    \"\"\"Download a Hugging Face model and tokenizer to the specified directory\"\"\"\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(model_path):\n",
    "        # Create the directory\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    # Save the model and tokenizer to the specified directory\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# For this demo, download the English-French and French-English models\n",
    "download_model('models/en_fr/', 'Helsinki-NLP/opus-mt-en-fr')\n",
    "download_model('models/fr_en/', 'Helsinki-NLP/opus-mt-fr-en')\n",
    "\n",
    "Creating the Flask translation web service endpoint\n",
    "The last thing we have to do with the Dockerfile is to expose the port where the web service will be available and to tell the container what to run when it starts:\n",
    "\n",
    "# Make port 6000 available to the world outside this container\n",
    "EXPOSE 6000\n",
    "\n",
    "ENTRYPOINT [ \"python3\" ]\n",
    "\n",
    "# Run main.py when the container launches\n",
    "CMD [ \"main.py\" ]\n",
    "\n",
    "We expose the port 6000 to the outside world, and we tell Docker to run the python3 command with main.py. The main.py file is a very simple file that register the web service’s path using Flask, and that makes the predictions (translations in this case):\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def get_model(model_path):\n",
    "    \"\"\"Load a Hugging Face model and tokenizer from the specified directory\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the models and tokenizers for each supported language\n",
    "en_fr_model, en_fr_tokenizer = get_model('models/en_fr/')\n",
    "fr_en_model, fr_en_tokenizer = get_model('models/fr_en/')\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def is_translation_supported(from_lang, to_lang):\n",
    "    \"\"\"Check if the specified translation is supported\"\"\"\n",
    "    supported_translations = ['en_fr', 'fr_en']\n",
    "    return f'{from_lang}_{to_lang}' in supported_translations\n",
    "\n",
    "@app.route('/translate/<from_lang>/<to_lang>/', methods=['POST'])\n",
    "def translate_endpoint(from_lang, to_lang):\n",
    "    \"\"\"Translate text from one language to another. This function is \n",
    "    called when a POST request is sent to /translate/<from_lang>/<to_lang>/\"\"\"\n",
    "    if not is_translation_supported(from_lang, to_lang):\n",
    "        return jsonify({'error': 'Translation not supported'}), 400\n",
    "\n",
    "    data = request.get_json()\n",
    "    from_text = data.get(f'{from_lang}_text', '')\n",
    "\n",
    "    if from_text:\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "\n",
    "        match from_lang:\n",
    "            case 'en':        \n",
    "                model = en_fr_model\n",
    "                tokenizer = en_fr_tokenizer\n",
    "            case 'fr':\n",
    "                model = fr_en_model\n",
    "                tokenizer = fr_en_tokenizer\n",
    "\n",
    "        to_text = tokenizer.decode(model.generate(tokenizer.encode(from_text, return_tensors='pt')).squeeze(), skip_special_tokens=True)\n",
    "\n",
    "        return jsonify({f'{to_lang}_text': to_text})\n",
    "    else:\n",
    "        return jsonify({'error': 'Text to translate not provided'}), 400\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=6000, debug=True)\n",
    "\n",
    "Building the container\n",
    "Now that the Dockerfile is completed, the next step is to use it to have Docker to build the actual image of the container. This is done using this command in the terminal:\n",
    "\n",
    "docker build -t localbuild:en_fr_translation_service .\n",
    "\n",
    "Note that we specified a tag to make it easier to manage it in between all the other images that may exists in the environment. The output of the terminal will show every step defined in the Dockerfile, and the processing for each of those step. The final output looks like:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=3122, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "tokenizer = Anthropic().get_tokenizer()\n",
    "tokenizer.encode(str(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13703"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_list = context.split('.')\n",
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runpod\n",
    "\n",
    "runpod.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "endpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n",
    "\n",
    "run_request = endpoint.run(\n",
    "    {\"input\": {\n",
    "        \"context\": context_list,\n",
    "        \"instruction\": \"You are a question answering bot who uses the provided context to answer a question\\n\",\n",
    "        \"question\": \"How many unique language-image instruction-following samples in total?\",\n",
    "        \"target_tokens\": 500,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_PROGRESS\n",
      "{'compressed_prompt': 'You are a question answering bot who uses the provided context to answer a question\\n\\n\\nruction tuning large models () using machine-generated\\ninstruction-ing data has been to improveshot capabilities on\\nnew tasks, but the idea is less explored in theal field\\n\\n We present the\\nfirst attempt to use language-onlyPT-4 to generateal language-image\\nructioning data\\n\\n Our experiments show that LLaVA demonstrates impressiveal chat abilities, sometimes exhibiting the behaviors\\nofalPT-4 on unseen images/ions, and yields a 85\\n\\n1% relative score compared with-4 on a synthetical instructioning\\net\\n\\n One of the core aspirations in intelligence\\nis to develop a generalpurpose assistant that can effectively follow-al vision-and-language\\nions, with human intent to complete variousworld tasks in the wild [4, 27, 26]\\n\\n Alpaca [48], Vic [9], GPT-4-LLM [38]\\n37th Conference on Neural Systems (IPS 2023)\\n\\nCV] 11 Dec 2023\\nutilize various machine-generated-quality instruction-ing samples to improve the LLMs\\nalignment ability, reporting impressive performance compared with proprietaryMs\\n\\n\\nIn this we present visual instruction-ing, the first attempt to extend-ing to\\nthe language-imageal space, to way towards building a generalpurpose visual\\nistant\\n\\n In, our paper makes following contributions:\\n•al-ing data\\n\\n One key challenge is the of-language\\nructioning data\\n\\n We present a data reformation perspective and pipeline to convert\\nimage-text pairs into an appropriate instructioning format, usingPT/-4\\n\\n We develop a largeal model (LMM), by connecting the\\n-set visualoder ofIP [40] with the languageoder Vicuna [9], and-ing\\nend on our generated instructional vision-language data\\n\\n\\n•al instruction-ing benchmark\\n\\n We release the following assets to the public: the generatedal instruction\\ndata, the codebase, the model, and a visual chat demo\\n\\n\\n2 Related Work\\nal Instruction-ing Agents\\n\\n In, existing works that build instructioning agents be broadly categorized into two: ()end trained models, which\\nare separately explored for each specific research topic\\n\\n For, the vision-language navigation\\ntask [3, 19] andat [] require the embodied AI to follow natural language instructions\\nand take a of actions to complete goals in visual environments\\n\\n While sharing the same goal in building instruction-ing\\n, we on developing anend trained-visional model for multiple\\nasks\\n\\n on the recent�best� open-source LLM LLaMA, OpenFlamingo [5] and\\nLLaMA- [] aresource efforts that enable to use image inputs, paving theway to buildsourcealMs\\n\\n While these models present promising task transfer\\nization, they are not explicitly tuned withlanguage instruction data, and\\ntheir performance inal tasks usually short compared toonly tasks\\n\\n, that visual instruction tuning\\nis from visual tuning [23]: former to improve the models instructioning abilities, while latter the parameter-efficiency in model adaptation\\n\\n\\n3-assisted Instruction Generation\\nThe community has witnessed a surge in the of publical data astext\\nairs, from CC [ to LA []\\n\\n914], suitcase: \\n\\n\\n\\n\\n\\n<\\nResponse type: conversation\\n: What of is the\\n: The image a black vehicleV) \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable 1: One example to the instruction-ing data\\n\\n by the\\n of recentPT models in text-otation [], we propose to leveragePT/4\\nforal-ing data collection, the widely existing-pair data\\n\\n, a simple to expand an pair to its\\n-following version is Human : Xq Xv<STOP> Assistant : Xc<STOP>\\n\\n\\nTo mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both\\naccept only text as input), to create instruction-following data involving visual content\\n\\n We\\nuse COCO images [31] and generate three types of instruction-following data\\n\\n We prompt GPT-4 then curate the list (see detailed prompts\\n3\\nand curation process in Appendix)\\n\\n\\nWe collect 158K unique language-image instruction-following samples in total, including 58K in\\nconversations, 23K in detailed description, and 77k in complex reasoning, respectively\\n\\nHow many unique language-image instruction-following samples in total?', 'compressed_tokens': 892, 'origin_tokens': 3087, 'ratio': '3.5x', 'saving': ', Saving $0.1 in GPT-4.'}\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "print(run_request.status())\n",
    "response = run_request.output(timeout=60)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compressed_prompt': 'You are a question answering bot who uses the provided '\n",
      "                      'context to answer a question\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      'ruction tuning large models () using machine-generated\\n'\n",
      "                      'instruction-ing data has been to improveshot '\n",
      "                      'capabilities on\\n'\n",
      "                      'new tasks, but the idea is less explored in theal '\n",
      "                      'field\\n'\n",
      "                      '\\n'\n",
      "                      ' We present the\\n'\n",
      "                      'first attempt to use language-onlyPT-4 to generateal '\n",
      "                      'language-image\\n'\n",
      "                      'ructioning data\\n'\n",
      "                      '\\n'\n",
      "                      ' Our experiments show that LLaVA demonstrates '\n",
      "                      'impressiveal chat abilities, sometimes exhibiting the '\n",
      "                      'behaviors\\n'\n",
      "                      'ofalPT-4 on unseen images/ions, and yields a 85\\n'\n",
      "                      '\\n'\n",
      "                      '1% relative score compared with-4 on a synthetical '\n",
      "                      'instructioning\\n'\n",
      "                      'et\\n'\n",
      "                      '\\n'\n",
      "                      ' One of the core aspirations in intelligence\\n'\n",
      "                      'is to develop a generalpurpose assistant that can '\n",
      "                      'effectively follow-al vision-and-language\\n'\n",
      "                      'ions, with human intent to complete variousworld tasks '\n",
      "                      'in the wild [4, 27, 26]\\n'\n",
      "                      '\\n'\n",
      "                      ' Alpaca [48], Vic [9], GPT-4-LLM [38]\\n'\n",
      "                      '37th Conference on Neural Systems (IPS 2023)\\n'\n",
      "                      '\\n'\n",
      "                      'CV] 11 Dec 2023\\n'\n",
      "                      'utilize various machine-generated-quality '\n",
      "                      'instruction-ing samples to improve the LLMs\\n'\n",
      "                      'alignment ability, reporting impressive performance '\n",
      "                      'compared with proprietaryMs\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      'In this we present visual instruction-ing, the first '\n",
      "                      'attempt to extend-ing to\\n'\n",
      "                      'the language-imageal space, to way towards building a '\n",
      "                      'generalpurpose visual\\n'\n",
      "                      'istant\\n'\n",
      "                      '\\n'\n",
      "                      ' In, our paper makes following contributions:\\n'\n",
      "                      '•al-ing data\\n'\n",
      "                      '\\n'\n",
      "                      ' One key challenge is the of-language\\n'\n",
      "                      'ructioning data\\n'\n",
      "                      '\\n'\n",
      "                      ' We present a data reformation perspective and pipeline '\n",
      "                      'to convert\\n'\n",
      "                      'image-text pairs into an appropriate instructioning '\n",
      "                      'format, usingPT/-4\\n'\n",
      "                      '\\n'\n",
      "                      ' We develop a largeal model (LMM), by connecting the\\n'\n",
      "                      '-set visualoder ofIP [40] with the languageoder Vicuna '\n",
      "                      '[9], and-ing\\n'\n",
      "                      'end on our generated instructional vision-language '\n",
      "                      'data\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '•al instruction-ing benchmark\\n'\n",
      "                      '\\n'\n",
      "                      ' We release the following assets to the public: the '\n",
      "                      'generatedal instruction\\n'\n",
      "                      'data, the codebase, the model, and a visual chat demo\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '2 Related Work\\n'\n",
      "                      'al Instruction-ing Agents\\n'\n",
      "                      '\\n'\n",
      "                      ' In, existing works that build instructioning agents be '\n",
      "                      'broadly categorized into two: ()end trained models, '\n",
      "                      'which\\n'\n",
      "                      'are separately explored for each specific research '\n",
      "                      'topic\\n'\n",
      "                      '\\n'\n",
      "                      ' For, the vision-language navigation\\n'\n",
      "                      'task [3, 19] andat [] require the embodied AI to follow '\n",
      "                      'natural language instructions\\n'\n",
      "                      'and take a of actions to complete goals in visual '\n",
      "                      'environments\\n'\n",
      "                      '\\n'\n",
      "                      ' While sharing the same goal in building '\n",
      "                      'instruction-ing\\n'\n",
      "                      ', we on developing anend trained-visional model for '\n",
      "                      'multiple\\n'\n",
      "                      'asks\\n'\n",
      "                      '\\n'\n",
      "                      ' on the recent�best� open-source LLM LLaMA, '\n",
      "                      'OpenFlamingo [5] and\\n'\n",
      "                      'LLaMA- [] aresource efforts that enable to use image '\n",
      "                      'inputs, paving theway to buildsourcealMs\\n'\n",
      "                      '\\n'\n",
      "                      ' While these models present promising task transfer\\n'\n",
      "                      'ization, they are not explicitly tuned withlanguage '\n",
      "                      'instruction data, and\\n'\n",
      "                      'their performance inal tasks usually short compared '\n",
      "                      'toonly tasks\\n'\n",
      "                      '\\n'\n",
      "                      ', that visual instruction tuning\\n'\n",
      "                      'is from visual tuning [23]: former to improve the '\n",
      "                      'models instructioning abilities, while latter the '\n",
      "                      'parameter-efficiency in model adaptation\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '3-assisted Instruction Generation\\n'\n",
      "                      'The community has witnessed a surge in the of publical '\n",
      "                      'data astext\\n'\n",
      "                      'airs, from CC [ to LA []\\n'\n",
      "                      '\\n'\n",
      "                      '914], suitcase: \\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '<\\n'\n",
      "                      'Response type: conversation\\n'\n",
      "                      ': What of is the\\n'\n",
      "                      ': The image a black vehicleV) \\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      'Table 1: One example to the instruction-ing data\\n'\n",
      "                      '\\n'\n",
      "                      ' by the\\n'\n",
      "                      ' of recentPT models in text-otation [], we propose to '\n",
      "                      'leveragePT/4\\n'\n",
      "                      'foral-ing data collection, the widely existing-pair '\n",
      "                      'data\\n'\n",
      "                      '\\n'\n",
      "                      ', a simple to expand an pair to its\\n'\n",
      "                      '-following version is Human : Xq Xv<STOP> Assistant : '\n",
      "                      'Xc<STOP>\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      'To mitigate this issue, we leverage language-only GPT-4 '\n",
      "                      'or ChatGPT as the strong teacher (both\\n'\n",
      "                      'accept only text as input), to create '\n",
      "                      'instruction-following data involving visual content\\n'\n",
      "                      '\\n'\n",
      "                      ' We\\n'\n",
      "                      'use COCO images [31] and generate three types of '\n",
      "                      'instruction-following data\\n'\n",
      "                      '\\n'\n",
      "                      ' We prompt GPT-4 then curate the list (see detailed '\n",
      "                      'prompts\\n'\n",
      "                      '3\\n'\n",
      "                      'and curation process in Appendix)\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      'We collect 158K unique language-image '\n",
      "                      'instruction-following samples in total, including 58K '\n",
      "                      'in\\n'\n",
      "                      'conversations, 23K in detailed description, and 77k in '\n",
      "                      'complex reasoning, respectively\\n'\n",
      "                      '\\n'\n",
      "                      'How many unique language-image instruction-following '\n",
      "                      'samples in total?',\n",
      " 'compressed_tokens': 892,\n",
      " 'origin_tokens': 3087,\n",
      " 'ratio': '3.5x',\n",
      " 'saving': ', Saving $0.1 in GPT-4.'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    client=bedrock,\n",
    "    model_kwargs={\n",
    "    \"temperature\":0.0,\n",
    "    \"max_tokens_to_sample\":50,\n",
    "    \"top_k\":5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many unique language-image instruction-following samples in total?\"\n",
    "prompt = f'''You are a question answering bot who uses ONLY the provided context <context>{context}<context> \n",
    "to answer a question in one concise sentence. <question>{question}<question>. If there is no answer, please write \"No answer\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "without_compression = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_compressed = f'''You are a question answering bot who uses ONLY the provided context \n",
    "<context>{response[\"compressed_prompt\"].replace(\"You are a question answering bot who uses the provided context to answer a question\", '')}<context> \n",
    "to answer a question in one concise sentence. <question>{question}<question>. If there is no answer, please write \"No answer\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with_compression = llm(prompt_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rouge_scorer.py     :83   2024-04-06 15:50:53,625 Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rougeL': Score(precision=1.0, recall=0.3, fmeasure=0.4615384615384615)}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(without_compression.strip(), with_compression.strip())\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' 158K unique language-image instruction-following samples in total.'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(with_compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Based on the provided context, there are 158K unique language-image '\n",
      " 'instruction-following samples collected in total, including 58K in '\n",
      " 'conversations, 23K in detailed description, and 77K in complex reasoning.')\n"
     ]
    }
   ],
   "source": [
    "pprint(without_compression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
