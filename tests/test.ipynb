{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Instruction tuning large language models (LLMs) using machine-generated\n",
    "instruction-following data has been shown to improve zero-shot capabilities on\n",
    "new tasks, but the idea is less explored in the multimodal field. We present the\n",
    "first attempt to use language-only GPT-4 to generate multimodal language-image\n",
    "instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained\n",
    "large multimodal model that connects a vision encoder and an LLM for generalpurpose visual and language understanding. To facilitate future research on visual\n",
    "instruction following, we construct two evaluation benchmarks with diverse and\n",
    "challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors\n",
    "of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following\n",
    "dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4\n",
    "achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated\n",
    "visual instruction tuning data, our model, and code publicly available.\n",
    "1 Introduction\n",
    "Humans interact with the world through many channels such as vision and language, as each\n",
    "individual channel has a unique advantage in representing and communicating certain concepts, and\n",
    "thus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence\n",
    "is to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language\n",
    "instructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26].\n",
    "To this end, the community has witnessed an emergent interest in developing language-augmented\n",
    "foundation vision models [27, 16], with strong capabilities in open-world visual understanding\n",
    "such as classification [40, 21, 57, 54, 39], detection [29, 62, 33], segmentation [25, 63, 58] and\n",
    "captioning [50, 28], as well as visual generation and editing [42, 43, 56, 15, 44, 30]. We refer readers\n",
    "to the Computer Vision in the Wild reading list for a more up-to-date literature compilation [12]. In\n",
    "this line of work, each task is solved independently by one single large vision model, with the task\n",
    "instruction implicitly considered in the model design. Further, language is only utilized to describe\n",
    "the image content. While this allows language to play an important role in mapping visual signals to\n",
    "language semantics—a common channel for human communication, it leads to models that usually\n",
    "have a fixed interface with limited interactivity and adaptability to the user’s instructions.\n",
    "Large language models (LLM), on the other hand, have shown that language can play a wider\n",
    "role: a universal interface for a general-purpose assistant, where various task instructions can be\n",
    "explicitly represented in language and guide the end-to-end trained neural assistant to switch to the\n",
    "task of interest to solve it. For example, the recent success of ChatGPT [35] and GPT-4 [36] have\n",
    "demonstrated the power of aligned LLMs in following human instructions, and have stimulated\n",
    "tremendous interest in developing open-source LLMs. Among them, LLaMA [49] is an opensource LLM that matches the performance of GPT-3. Alpaca [48], Vicuna [9], GPT-4-LLM [38]\n",
    "37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n",
    "arXiv:2304.08485v2 [cs.CV] 11 Dec 2023\n",
    "utilize various machine-generated high-quality instruction-following samples to improve the LLM’s\n",
    "alignment ability, reporting impressive performance compared with proprietary LLMs. Importantly,\n",
    "this line of work is text-only.\n",
    "In this paper, we present visual instruction-tuning, the first attempt to extend instruction-tuning to\n",
    "the language-image multimodal space, to pave the way towards building a general-purpose visual\n",
    "assistant. In particular, our paper makes the following contributions:\n",
    "• Multimodal instruction-following data. One key challenge is the lack of vision-language\n",
    "instruction-following data. We present a data reformation perspective and pipeline to convert\n",
    "image-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4.\n",
    "• Large multimodal models. We develop a large multimodal model (LMM), by connecting the\n",
    "open-set visual encoder of CLIP [40] with the language decoder Vicuna [9], and fine-tuning\n",
    "end-to-end on our generated instructional vision-language data. Our empirical study validates\n",
    "the effectiveness of using generated data for LMM instruction-tuning, and suggests practical\n",
    "tips for building a general-purpose instruction-following visual agent. When ensembled with\n",
    "GPT-4, our approach achieves SoTA on the Science QA [34] multimodal reasoning dataset.\n",
    "• Multimodal instruction-following benchmark. We present LLaVA-Bench with two challenging\n",
    "benchmarks, with a diverse selection of paired images, instructions and detailed annotations.\n",
    "• Open-source. We release the following assets to the public: the generated multimodal instruction\n",
    "data, the codebase, the model checkpoints, and a visual chat demo.\n",
    "2 Related Work\n",
    "Multimodal Instruction-following Agents. In computer vision, existing works that build instructionfollowing agents can be broadly categorized into two classes: (i) End-to-end trained models, which\n",
    "are separately explored for each specific research topic. For example, the vision-language navigation\n",
    "task [3, 19] and Habitat [47] require the embodied AI agent to follow natural language instructions\n",
    "and take a sequence of actions to complete goals in visual environments. In the image editing domain,\n",
    "given an input image and a written instruction that tells the agent what to do, InstructPix2Pix [6]\n",
    "edits images by following the human instructions. (ii) A system that coordinates various models\n",
    "via LangChain [1] / LLMs [35], such as Visual ChatGPT [53], X-GPT [63], MM-REACT [55],\n",
    "VisProg [18], and ViperGPT [46]. While sharing the same goal in building instruction-following\n",
    "agents, we focus on developing an end-to-end trained language-vision multimodal model for multiple\n",
    "tasks.\n",
    "Instruction Tuning. In the natural language processing (NLP) community, to enable LLMs such\n",
    "as GPT-3 [7], T5 [41], PaLM [10], and OPT [60] to follow natural language instructions and\n",
    "complete real-world tasks, researchers have explored methods for LLM instruction-tuning [37, 52, 51],\n",
    "leading to instruction-tuned counterparts such as InstructGPT [37]/ChatGPT [35], FLAN-T5 [11],\n",
    "FLAN-PaLM [11], and OPT-IML [22], respectively. It turns out that this simple approach can\n",
    "effectively improve the zero- and few-shot generalization abilities of LLMs. It is thus natural\n",
    "to borrow the idea from NLP to computer vision. More broadly, the teacher-student distillation\n",
    "ideas with foundation models have been studied in other topics such as image classification [14].\n",
    "Flamingo [2] can be viewed as the GPT-3 moment in the multimodal domain, due to its strong\n",
    "performance on zero-shot task transfer and in-context-learning. Other LMMs trained on imagetext pairs include BLIP-2 [28], FROMAGe [24], and KOSMOS-1 [20]. PaLM-E [13] is an LMM\n",
    "for embodied AI. Based on the recent “best” open-source LLM LLaMA, OpenFlamingo [5] and\n",
    "LLaMA-Adapter [59] are open-source efforts that enable LLaMA to use image inputs, paving the\n",
    "way to build open-source multimodal LLMs. While these models present promising task transfer\n",
    "generalization performance, they are not explicitly tuned with vision-language instruction data, and\n",
    "their performance in multimodal tasks usually falls short compared to language-only tasks. In this\n",
    "paper, we aim to fill this gap and study its effectiveness. Finally, note that visual instruction tuning\n",
    "is different from visual prompt tuning [23]: the former aims to improve the model’s instructionfollowing abilities, while the latter aims to improve the parameter-efficiency in model adaptation.\n",
    "3 GPT-assisted Visual Instruction Data Generation\n",
    "The community has witnessed a surge in the amount of public multimodal data such as image-text\n",
    "pairs, ranging from CC [8] to LAION [45]. However, when it comes to multimodal instruction2\n",
    "Context type 1: Captions\n",
    "A group of people standing outside of a black vehicle with various luggage.\n",
    "Luggage surrounds a vehicle in an underground parking area\n",
    "People try to fit all of their luggage in an SUV.\n",
    "The sport utility vehicle is parked in the public garage, being packed for a trip\n",
    "Some people with luggage near a van that is transporting it.\n",
    "Context type 2: Boxes\n",
    "person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ...<omitted>\n",
    "Response type 1: conversation\n",
    "Question: What type of vehicle is featured in the image?\n",
    "Answer: The image features a black sport utility vehicle (SUV) ...<omitted>\n",
    "Response type 2: detailed description\n",
    "The image is an underground parking area with a black sport utility vehicle (SUV) parked. There\n",
    "are three people in the scene, with one person standing closer to the left side of the vehicle, another\n",
    "person in the middle, and the third person on the right side. They are all working together to pack\n",
    "their luggage into the SUV for a trip. ...<omitted>\n",
    "Response type 3: complex reasoning\n",
    "Question: What challenges do these people face?\n",
    "Answer: In the image, a group of people is standing outside a black SUV in a parking area,\n",
    "surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the\n",
    "challenge of fitting all their luggage into the black SUV. There are multiple suitcases and backpacks\n",
    "to be packed, which suggests that the group has a significant amount of belongings ...<omitted>\n",
    "Table 1: One example to illustrate the instruction-following data. The top block shows the contexts\n",
    "such as captions and boxes used to prompt GPT, and the bottom block shows the three types of\n",
    "responses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.\n",
    "following data, the available amount is limited, partially because the process for creating such data is\n",
    "time-consuming and less well-defined when human crowd-scouring is considered. Inspired by the\n",
    "success of recent GPT models in text-annotation tasks [17], we propose to leverage ChatGPT/GPT-4\n",
    "for multimodal instruction-following data collection, based on the widely existing image-pair data.\n",
    "For an image Xv and its associated caption Xc, it is natural to create a set of questions Xq with the\n",
    "intent to instruct the assistant to describe the image content. We prompt GPT-4 to curate such a list\n",
    "of questions (see details in Appendix). Therefore, a simple way to expand an image-text pair to its\n",
    "instruction-following version is Human : Xq Xv<STOP> Assistant : Xc<STOP>. Though cheap to\n",
    "construct, this simple expanded version lacks diversity and in-depth reasoning in both the instructions\n",
    "and responses.\n",
    "To mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both\n",
    "accept only text as input), to create instruction-following data involving visual content. Specifically,\n",
    "in order to encode an image into its visual features to prompt a text-only GPT, we use two types of\n",
    "symbolic representations: (i) Captions typically describe the visual scene from various perspectives;\n",
    "(ii) Bounding boxes usually localize the objects in the scene, and each box encodes the object concept\n",
    "and its spatial location. One example is shown in the top block of Table 14.\n",
    "This symbolic representation allows us to encode the image as an LLM-recognizable sequence. We\n",
    "use COCO images [31] and generate three types of instruction-following data. One example per type\n",
    "is shown in the bottom block of Table 14. For each type, we first manually design a few examples.\n",
    "They are the only human annotations we have during data collection, and are used as seed examples\n",
    "in in-context-learning to query GPT-4.\n",
    "• Conversation. We design a conversation between the assistant and a person asking questions\n",
    "about this photo. The answers are in a tone as if the assistant is seeing the image and answering\n",
    "the question. A diverse set of questions are asked about the visual content of the image, including\n",
    "the object types, counting the objects, object actions, object locations, relative positions between\n",
    "objects. Only questions that have definite answers are considered. Please see Appendix for the\n",
    "detailed prompt.\n",
    "• Detailed description. To include a rich and comprehensive description for an image, we create a\n",
    "list of questions with such an intent. We prompt GPT-4 then curate the list (see detailed prompts\n",
    "3\n",
    "and curation process in Appendix). For each image, we randomly sample one question from the\n",
    "list to ask GPT-4 to generate the detailed description.\n",
    "• Complex reasoning. The above two types focus on the visual content itself, based on which\n",
    "we further create in-depth reasoning questions. The answers typically require a step-by-step\n",
    "reasoning process by following rigorous logic.\n",
    "We collect 158K unique language-image instruction-following samples in total, including 58K in\n",
    "conversations, 23K in detailed description, and 77k in complex reasoning, respectively. We ablated\n",
    "the use of ChatGPT and GPT-4 in our early experiments, and found that GPT-4 consistently provides\n",
    "higher quality instruction-following data, such as spatial reasoning.\n",
    "4 Visual Instruction Tuning\n",
    "4.1 Architecture\n",
    "The primary goal is to effectively leverage the capabilities of both the pre-trained LLM and visual\n",
    "model. The network archtecture is illustrated in Figure 1. We choose Vicuna [9] as our LLM fϕ(·)\n",
    "parameterized by ϕ, as it has the best instruction following capabilities in language tasks among\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = context.split('.')\n",
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runpod\n",
    "\n",
    "runpod.api_key = \"YOUR_RUNPOD_API_KEY\"\n",
    "\n",
    "endpoint = runpod.Endpoint(\"ENDPOINT_ID\")\n",
    "\n",
    "run_request = endpoint.run(\n",
    "    {\"input\": {\n",
    "        \"context\": context,\n",
    "        \"instruction\": \"You are a question answering bot who uses the provided context to answer a question\\n\",\n",
    "        \"question\": \"<question>What abilities does LLaVa demonstrate?</question>\",\n",
    "        \"target_tokens\": 300,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_QUEUE\n",
      "{'compressed_prompt': 'You are a question answering bot who uses the provided context to answer a question\\n\\n\\n By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, anend trained\\nlargeal model that connects a visionoder and an LLM for generalpurpose visual and language understanding\\n\\n Our experiments thatVA demonstrates impressiveal chat abilities, sometimes exhibiting the behaviors\\nofalPT-4 on unseen images/ions, and yields a 85\\n\\n Whentuned on Science QA, the synergy ofVA and4\\nieves a newart accuracy of 92\\n\\n We make4 generated\\nvisual instruction tuning data, our model, and code available\\n\\n One the core aspirations in intelligence\\nis to develop a generalpurpose assistant that can effectively follow-al vision--languageions, with human intent to complete various tasks in the wild [4, 27, 26]\\n\\n We refer readersto the Vision in the Wild reading list for a moredate literature compilation [12]\\n\\n Further, language is only utilized to describe\\nthe image content\\n\\n\\nLarge modelsLLM),, have shown that language can play a wider\\nrole: a universal interface for a generalpurpose assistant, where various task instructions be\\nly represented in language and the end-end trained neural assistant to switch to the\\ntask of interest to solve it\\n\\n Among, LLaMA [] is anourceM that matches the of3\\n\\n Alpaca [48], Vic [], G-4- [38]\\n37th on SystemsIPS 20)\\n\\nCV] 11 Dec23\\nutilize various machine-generated-quality instruction-ing samples to improve theMs\\nalignment ability, reporting impressive performance compared with proprietaryMs\\n\\n We presentVA-Bench with two challenging\\nmarks, with a diverse of paired images, instructions and detailed annotations\\n\\n We release the following assets to the: the generatedal instruction\\ndata, the codebase, the model and a visual chat demo\\n\\n\\n2 Work\\nal Instruction-ing Agents\\n\\n In, existing works that build instructioning agents be broadly into two: ()end trained models, which\\nare separately explored for each specific research topic\\n\\n In the image editing,\\ngiven an image and a instruction that tells the agent do, InstructPix []\\nits images by the human instructions\\n\\n) A that coordinates various models\\nvia Lang [1] / LL [35], as Visual Chat [53], X- [], MM-RE [],\\nVisPro [], and Viper [\\n\\n While sharing the same goal in building instruction-ing, we on developingend trained languagevisional model for multiple\\nasks\\n\\n It is thus naturalto borrow the idea fromLP to vision\\n\\n PaLM- [] is an LMM\\nfor embodied AI\\n\\n on the recent�best� open-sourceM LMA, OpenFlam [] and\\nLLa- [] aresource efforts that enable to use image inputs,way to buildsourcealMs\\n914], suitcase: \\n\\n\\n\\n\\n\\n are multiplecases andpacks\\nto be packed, which suggests that the group has a significant of belongings\\n\\n\\n<\\nTable: One example to the instruction-ing data\\n\\n Note the visual image is not used to promptPT, only show it here a reference\\nFor an image Xv and its associated caption, is to a of questionsq with the\\n to the to image content\\n\\n We prompt-4 toate a list\\n questions (see in Appendix\\n\\n, a simple to expand an image- pair to its\\ning is Human : Xq Xv<ST> Assistantc\\n\\n,\\nin to encode image into its visual features to prompt a text-onlyPT use two of\\n: ()ions typically describe the visual scene from various perspectives;\\n(ii) Bounding boxes usually localize the objects in the scene, and each box encodes the object concept\\nand its spatial location\\n\\n The answers are in a tone as if the assistant is seeing the image and answering\\nthe question\\n\\n To include a rich and comprehensive description for an image, we create a\\nlist of questions with such an intent\\n\\n We prompt GPT-4 then curate the list (see detailed prompts\\n3\\nand curation process in Appendix)\\n\\n The above two types focus on the visual content itself, based on which\\nwe further create in-depth reasoning questions\\n\\n1 Architecture\\nThe primary goal is to effectively leverage the capabilities of both the pre-trained LLM and visual\\nmodel\\n\\n<question>What abilities does LLaVa demonstrate?</question>', 'compressed_tokens': 885, 'origin_tokens': 3088, 'ratio': '3.5x', 'saving': ', Saving $0.1 in GPT-4.'}\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "print(run_request.status())\n",
    "response = run_request.output(timeout=60)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compressed_prompt': 'You are a question answering bot who uses the provided '\n",
      "                      'context to answer a question\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      ' By instruction tuning on such generated data, we '\n",
      "                      'introduce LLaVA: Large Language and Vision Assistant, '\n",
      "                      'anend trained\\n'\n",
      "                      'largeal model that connects a visionoder and an LLM for '\n",
      "                      'generalpurpose visual and language understanding\\n'\n",
      "                      '\\n'\n",
      "                      ' Our experiments thatVA demonstrates impressiveal chat '\n",
      "                      'abilities, sometimes exhibiting the behaviors\\n'\n",
      "                      'ofalPT-4 on unseen images/ions, and yields a 85\\n'\n",
      "                      '\\n'\n",
      "                      ' Whentuned on Science QA, the synergy ofVA and4\\n'\n",
      "                      'ieves a newart accuracy of 92\\n'\n",
      "                      '\\n'\n",
      "                      ' We make4 generated\\n'\n",
      "                      'visual instruction tuning data, our model, and code '\n",
      "                      'available\\n'\n",
      "                      '\\n'\n",
      "                      ' One the core aspirations in intelligence\\n'\n",
      "                      'is to develop a generalpurpose assistant that can '\n",
      "                      'effectively follow-al vision--languageions, with human '\n",
      "                      'intent to complete various tasks in the wild [4, 27, '\n",
      "                      '26]\\n'\n",
      "                      '\\n'\n",
      "                      ' We refer readersto the Vision in the Wild reading list '\n",
      "                      'for a moredate literature compilation [12]\\n'\n",
      "                      '\\n'\n",
      "                      ' Further, language is only utilized to describe\\n'\n",
      "                      'the image content\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      'Large modelsLLM),, have shown that language can play a '\n",
      "                      'wider\\n'\n",
      "                      'role: a universal interface for a generalpurpose '\n",
      "                      'assistant, where various task instructions be\\n'\n",
      "                      'ly represented in language and the end-end trained '\n",
      "                      'neural assistant to switch to the\\n'\n",
      "                      'task of interest to solve it\\n'\n",
      "                      '\\n'\n",
      "                      ' Among, LLaMA [] is anourceM that matches the of3\\n'\n",
      "                      '\\n'\n",
      "                      ' Alpaca [48], Vic [], G-4- [38]\\n'\n",
      "                      '37th on SystemsIPS 20)\\n'\n",
      "                      '\\n'\n",
      "                      'CV] 11 Dec23\\n'\n",
      "                      'utilize various machine-generated-quality '\n",
      "                      'instruction-ing samples to improve theMs\\n'\n",
      "                      'alignment ability, reporting impressive performance '\n",
      "                      'compared with proprietaryMs\\n'\n",
      "                      '\\n'\n",
      "                      ' We presentVA-Bench with two challenging\\n'\n",
      "                      'marks, with a diverse of paired images, instructions '\n",
      "                      'and detailed annotations\\n'\n",
      "                      '\\n'\n",
      "                      ' We release the following assets to the: the '\n",
      "                      'generatedal instruction\\n'\n",
      "                      'data, the codebase, the model and a visual chat demo\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '2 Work\\n'\n",
      "                      'al Instruction-ing Agents\\n'\n",
      "                      '\\n'\n",
      "                      ' In, existing works that build instructioning agents be '\n",
      "                      'broadly into two: ()end trained models, which\\n'\n",
      "                      'are separately explored for each specific research '\n",
      "                      'topic\\n'\n",
      "                      '\\n'\n",
      "                      ' In the image editing,\\n'\n",
      "                      'given an image and a instruction that tells the agent '\n",
      "                      'do, InstructPix []\\n'\n",
      "                      'its images by the human instructions\\n'\n",
      "                      '\\n'\n",
      "                      ') A that coordinates various models\\n'\n",
      "                      'via Lang [1] / LL [35], as Visual Chat [53], X- [], '\n",
      "                      'MM-RE [],\\n'\n",
      "                      'VisPro [], and Viper [\\n'\n",
      "                      '\\n'\n",
      "                      ' While sharing the same goal in building '\n",
      "                      'instruction-ing, we on developingend trained '\n",
      "                      'languagevisional model for multiple\\n'\n",
      "                      'asks\\n'\n",
      "                      '\\n'\n",
      "                      ' It is thus naturalto borrow the idea fromLP to vision\\n'\n",
      "                      '\\n'\n",
      "                      ' PaLM- [] is an LMM\\n'\n",
      "                      'for embodied AI\\n'\n",
      "                      '\\n'\n",
      "                      ' on the recent�best� open-sourceM LMA, OpenFlam [] and\\n'\n",
      "                      'LLa- [] aresource efforts that enable to use image '\n",
      "                      'inputs,way to buildsourcealMs\\n'\n",
      "                      '914], suitcase: \\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      ' are multiplecases andpacks\\n'\n",
      "                      'to be packed, which suggests that the group has a '\n",
      "                      'significant of belongings\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '<\\n'\n",
      "                      'Table: One example to the instruction-ing data\\n'\n",
      "                      '\\n'\n",
      "                      ' Note the visual image is not used to promptPT, only '\n",
      "                      'show it here a reference\\n'\n",
      "                      'For an image Xv and its associated caption, is to a of '\n",
      "                      'questionsq with the\\n'\n",
      "                      ' to the to image content\\n'\n",
      "                      '\\n'\n",
      "                      ' We prompt-4 toate a list\\n'\n",
      "                      ' questions (see in Appendix\\n'\n",
      "                      '\\n'\n",
      "                      ', a simple to expand an image- pair to its\\n'\n",
      "                      'ing is Human : Xq Xv<ST> Assistantc\\n'\n",
      "                      '\\n'\n",
      "                      ',\\n'\n",
      "                      'in to encode image into its visual features to prompt a '\n",
      "                      'text-onlyPT use two of\\n'\n",
      "                      ': ()ions typically describe the visual scene from '\n",
      "                      'various perspectives;\\n'\n",
      "                      '(ii) Bounding boxes usually localize the objects in the '\n",
      "                      'scene, and each box encodes the object concept\\n'\n",
      "                      'and its spatial location\\n'\n",
      "                      '\\n'\n",
      "                      ' The answers are in a tone as if the assistant is '\n",
      "                      'seeing the image and answering\\n'\n",
      "                      'the question\\n'\n",
      "                      '\\n'\n",
      "                      ' To include a rich and comprehensive description for an '\n",
      "                      'image, we create a\\n'\n",
      "                      'list of questions with such an intent\\n'\n",
      "                      '\\n'\n",
      "                      ' We prompt GPT-4 then curate the list (see detailed '\n",
      "                      'prompts\\n'\n",
      "                      '3\\n'\n",
      "                      'and curation process in Appendix)\\n'\n",
      "                      '\\n'\n",
      "                      ' The above two types focus on the visual content '\n",
      "                      'itself, based on which\\n'\n",
      "                      'we further create in-depth reasoning questions\\n'\n",
      "                      '\\n'\n",
      "                      '1 Architecture\\n'\n",
      "                      'The primary goal is to effectively leverage the '\n",
      "                      'capabilities of both the pre-trained LLM and visual\\n'\n",
      "                      'model\\n'\n",
      "                      '\\n'\n",
      "                      '<question>What abilities does LLaVa '\n",
      "                      'demonstrate?</question>',\n",
      " 'compressed_tokens': 885,\n",
      " 'origin_tokens': 3088,\n",
      " 'ratio': '3.5x',\n",
      " 'saving': ', Saving $0.1 in GPT-4.'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
