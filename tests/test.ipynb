{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Instruction tuning large language models (LLMs) using machine-generated\n",
    "instruction-following data has been shown to improve zero-shot capabilities on\n",
    "new tasks, but the idea is less explored in the multimodal field. We present the\n",
    "first attempt to use language-only GPT-4 to generate multimodal language-image\n",
    "instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained\n",
    "large multimodal model that connects a vision encoder and an LLM for generalpurpose visual and language understanding. To facilitate future research on visual\n",
    "instruction following, we construct two evaluation benchmarks with diverse and\n",
    "challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors\n",
    "of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following\n",
    "dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4\n",
    "achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated\n",
    "visual instruction tuning data, our model, and code publicly available.\n",
    "1 Introduction\n",
    "Humans interact with the world through many channels such as vision and language, as each\n",
    "individual channel has a unique advantage in representing and communicating certain concepts, and\n",
    "thus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence\n",
    "is to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language\n",
    "instructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26].\n",
    "To this end, the community has witnessed an emergent interest in developing language-augmented\n",
    "foundation vision models [27, 16], with strong capabilities in open-world visual understanding\n",
    "such as classification [40, 21, 57, 54, 39], detection [29, 62, 33], segmentation [25, 63, 58] and\n",
    "captioning [50, 28], as well as visual generation and editing [42, 43, 56, 15, 44, 30]. We refer readers\n",
    "to the Computer Vision in the Wild reading list for a more up-to-date literature compilation [12]. In\n",
    "this line of work, each task is solved independently by one single large vision model, with the task\n",
    "instruction implicitly considered in the model design. Further, language is only utilized to describe\n",
    "the image content. While this allows language to play an important role in mapping visual signals to\n",
    "language semantics—a common channel for human communication, it leads to models that usually\n",
    "have a fixed interface with limited interactivity and adaptability to the user’s instructions.\n",
    "Large language models (LLM), on the other hand, have shown that language can play a wider\n",
    "role: a universal interface for a general-purpose assistant, where various task instructions can be\n",
    "explicitly represented in language and guide the end-to-end trained neural assistant to switch to the\n",
    "task of interest to solve it. For example, the recent success of ChatGPT [35] and GPT-4 [36] have\n",
    "demonstrated the power of aligned LLMs in following human instructions, and have stimulated\n",
    "tremendous interest in developing open-source LLMs. Among them, LLaMA [49] is an opensource LLM that matches the performance of GPT-3. Alpaca [48], Vicuna [9], GPT-4-LLM [38]\n",
    "37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n",
    "arXiv:2304.08485v2 [cs.CV] 11 Dec 2023\n",
    "utilize various machine-generated high-quality instruction-following samples to improve the LLM’s\n",
    "alignment ability, reporting impressive performance compared with proprietary LLMs. Importantly,\n",
    "this line of work is text-only.\n",
    "In this paper, we present visual instruction-tuning, the first attempt to extend instruction-tuning to\n",
    "the language-image multimodal space, to pave the way towards building a general-purpose visual\n",
    "assistant. In particular, our paper makes the following contributions:\n",
    "• Multimodal instruction-following data. One key challenge is the lack of vision-language\n",
    "instruction-following data. We present a data reformation perspective and pipeline to convert\n",
    "image-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4.\n",
    "• Large multimodal models. We develop a large multimodal model (LMM), by connecting the\n",
    "open-set visual encoder of CLIP [40] with the language decoder Vicuna [9], and fine-tuning\n",
    "end-to-end on our generated instructional vision-language data. Our empirical study validates\n",
    "the effectiveness of using generated data for LMM instruction-tuning, and suggests practical\n",
    "tips for building a general-purpose instruction-following visual agent. When ensembled with\n",
    "GPT-4, our approach achieves SoTA on the Science QA [34] multimodal reasoning dataset.\n",
    "• Multimodal instruction-following benchmark. We present LLaVA-Bench with two challenging\n",
    "benchmarks, with a diverse selection of paired images, instructions and detailed annotations.\n",
    "• Open-source. We release the following assets to the public: the generated multimodal instruction\n",
    "data, the codebase, the model checkpoints, and a visual chat demo.\n",
    "2 Related Work\n",
    "Multimodal Instruction-following Agents. In computer vision, existing works that build instructionfollowing agents can be broadly categorized into two classes: (i) End-to-end trained models, which\n",
    "are separately explored for each specific research topic. For example, the vision-language navigation\n",
    "task [3, 19] and Habitat [47] require the embodied AI agent to follow natural language instructions\n",
    "and take a sequence of actions to complete goals in visual environments. In the image editing domain,\n",
    "given an input image and a written instruction that tells the agent what to do, InstructPix2Pix [6]\n",
    "edits images by following the human instructions. (ii) A system that coordinates various models\n",
    "via LangChain [1] / LLMs [35], such as Visual ChatGPT [53], X-GPT [63], MM-REACT [55],\n",
    "VisProg [18], and ViperGPT [46]. While sharing the same goal in building instruction-following\n",
    "agents, we focus on developing an end-to-end trained language-vision multimodal model for multiple\n",
    "tasks.\n",
    "Instruction Tuning. In the natural language processing (NLP) community, to enable LLMs such\n",
    "as GPT-3 [7], T5 [41], PaLM [10], and OPT [60] to follow natural language instructions and\n",
    "complete real-world tasks, researchers have explored methods for LLM instruction-tuning [37, 52, 51],\n",
    "leading to instruction-tuned counterparts such as InstructGPT [37]/ChatGPT [35], FLAN-T5 [11],\n",
    "FLAN-PaLM [11], and OPT-IML [22], respectively. It turns out that this simple approach can\n",
    "effectively improve the zero- and few-shot generalization abilities of LLMs. It is thus natural\n",
    "to borrow the idea from NLP to computer vision. More broadly, the teacher-student distillation\n",
    "ideas with foundation models have been studied in other topics such as image classification [14].\n",
    "Flamingo [2] can be viewed as the GPT-3 moment in the multimodal domain, due to its strong\n",
    "performance on zero-shot task transfer and in-context-learning. Other LMMs trained on imagetext pairs include BLIP-2 [28], FROMAGe [24], and KOSMOS-1 [20]. PaLM-E [13] is an LMM\n",
    "for embodied AI. Based on the recent “best” open-source LLM LLaMA, OpenFlamingo [5] and\n",
    "LLaMA-Adapter [59] are open-source efforts that enable LLaMA to use image inputs, paving the\n",
    "way to build open-source multimodal LLMs. While these models present promising task transfer\n",
    "generalization performance, they are not explicitly tuned with vision-language instruction data, and\n",
    "their performance in multimodal tasks usually falls short compared to language-only tasks. In this\n",
    "paper, we aim to fill this gap and study its effectiveness. Finally, note that visual instruction tuning\n",
    "is different from visual prompt tuning [23]: the former aims to improve the model’s instructionfollowing abilities, while the latter aims to improve the parameter-efficiency in model adaptation.\n",
    "3 GPT-assisted Visual Instruction Data Generation\n",
    "The community has witnessed a surge in the amount of public multimodal data such as image-text\n",
    "pairs, ranging from CC [8] to LAION [45]. However, when it comes to multimodal instruction2\n",
    "Context type 1: Captions\n",
    "A group of people standing outside of a black vehicle with various luggage.\n",
    "Luggage surrounds a vehicle in an underground parking area\n",
    "People try to fit all of their luggage in an SUV.\n",
    "The sport utility vehicle is parked in the public garage, being packed for a trip\n",
    "Some people with luggage near a van that is transporting it.\n",
    "Context type 2: Boxes\n",
    "person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ...<omitted>\n",
    "Response type 1: conversation\n",
    "Question: What type of vehicle is featured in the image?\n",
    "Answer: The image features a black sport utility vehicle (SUV) ...<omitted>\n",
    "Response type 2: detailed description\n",
    "The image is an underground parking area with a black sport utility vehicle (SUV) parked. There\n",
    "are three people in the scene, with one person standing closer to the left side of the vehicle, another\n",
    "person in the middle, and the third person on the right side. They are all working together to pack\n",
    "their luggage into the SUV for a trip. ...<omitted>\n",
    "Response type 3: complex reasoning\n",
    "Question: What challenges do these people face?\n",
    "Answer: In the image, a group of people is standing outside a black SUV in a parking area,\n",
    "surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the\n",
    "challenge of fitting all their luggage into the black SUV. There are multiple suitcases and backpacks\n",
    "to be packed, which suggests that the group has a significant amount of belongings ...<omitted>\n",
    "Table 1: One example to illustrate the instruction-following data. The top block shows the contexts\n",
    "such as captions and boxes used to prompt GPT, and the bottom block shows the three types of\n",
    "responses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.\n",
    "following data, the available amount is limited, partially because the process for creating such data is\n",
    "time-consuming and less well-defined when human crowd-scouring is considered. Inspired by the\n",
    "success of recent GPT models in text-annotation tasks [17], we propose to leverage ChatGPT/GPT-4\n",
    "for multimodal instruction-following data collection, based on the widely existing image-pair data.\n",
    "For an image Xv and its associated caption Xc, it is natural to create a set of questions Xq with the\n",
    "intent to instruct the assistant to describe the image content. We prompt GPT-4 to curate such a list\n",
    "of questions (see details in Appendix). Therefore, a simple way to expand an image-text pair to its\n",
    "instruction-following version is Human : Xq Xv<STOP> Assistant : Xc<STOP>. Though cheap to\n",
    "construct, this simple expanded version lacks diversity and in-depth reasoning in both the instructions\n",
    "and responses.\n",
    "To mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both\n",
    "accept only text as input), to create instruction-following data involving visual content. Specifically,\n",
    "in order to encode an image into its visual features to prompt a text-only GPT, we use two types of\n",
    "symbolic representations: (i) Captions typically describe the visual scene from various perspectives;\n",
    "(ii) Bounding boxes usually localize the objects in the scene, and each box encodes the object concept\n",
    "and its spatial location. One example is shown in the top block of Table 14.\n",
    "This symbolic representation allows us to encode the image as an LLM-recognizable sequence. We\n",
    "use COCO images [31] and generate three types of instruction-following data. One example per type\n",
    "is shown in the bottom block of Table 14. For each type, we first manually design a few examples.\n",
    "They are the only human annotations we have during data collection, and are used as seed examples\n",
    "in in-context-learning to query GPT-4.\n",
    "• Conversation. We design a conversation between the assistant and a person asking questions\n",
    "about this photo. The answers are in a tone as if the assistant is seeing the image and answering\n",
    "the question. A diverse set of questions are asked about the visual content of the image, including\n",
    "the object types, counting the objects, object actions, object locations, relative positions between\n",
    "objects. Only questions that have definite answers are considered. Please see Appendix for the\n",
    "detailed prompt.\n",
    "• Detailed description. To include a rich and comprehensive description for an image, we create a\n",
    "list of questions with such an intent. We prompt GPT-4 then curate the list (see detailed prompts\n",
    "3\n",
    "and curation process in Appendix). For each image, we randomly sample one question from the\n",
    "list to ask GPT-4 to generate the detailed description.\n",
    "• Complex reasoning. The above two types focus on the visual content itself, based on which\n",
    "we further create in-depth reasoning questions. The answers typically require a step-by-step\n",
    "reasoning process by following rigorous logic.\n",
    "We collect 158K unique language-image instruction-following samples in total, including 58K in\n",
    "conversations, 23K in detailed description, and 77k in complex reasoning, respectively. We ablated\n",
    "the use of ChatGPT and GPT-4 in our early experiments, and found that GPT-4 consistently provides\n",
    "higher quality instruction-following data, such as spatial reasoning.\n",
    "4 Visual Instruction Tuning\n",
    "4.1 Architecture\n",
    "The primary goal is to effectively leverage the capabilities of both the pre-trained LLM and visual\n",
    "model. The network archtecture is illustrated in Figure 1. We choose Vicuna [9] as our LLM fϕ(·)\n",
    "parameterized by ϕ, as it has the best instruction following capabilities in language tasks among\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "context2 = '''In this short tutorial, we will explore how Hugging Face models can be deployed in a Docker Container and exposed as a web service endpoint.\n",
    "\n",
    "The service it exposes is a translation service from English to French and French to English.\n",
    "\n",
    "Why someone would like to do that? Other than to learn about those specific technologies, it is a very convenient way to try and test the thousands of models that exists on Hugging Face, in a clean and isolated environment that can easily be replicated, shared or deployed elsewhere than on your local computer.\n",
    "\n",
    "In this tutorial, you will learn how to use Docker to create a container with all the necessary code and artifacts to load Hugging Face models and to expose them as web service endpoints using Flask.\n",
    "\n",
    "All code and configurations used to write this blog post are available in this GitHub Repository. You simply have to clone it and to run the commands listed in this tutorial to replicate the service on your local machine.\n",
    "\n",
    "Installing Docker\n",
    "The first step is to install Docker. The easiest way is by simply installing Docker Desktop which is available on MacOS, Windows and Linux.\n",
    "\n",
    "Creating the Dockerfile\n",
    "The next step is to create a new Git repository where you will create a Dockerfile. The Dockerfile is where all instructions are written that tells Docker how to create the container.\n",
    "\n",
    "I would also strongly encourage you to install and use hadolint, which is a really good Docker linter that helps people to follow Docker best practices. There is also a plugin for VS Code if this is what you use as you development IDE.\n",
    "\n",
    "Base image and key installs\n",
    "The first thing you define in a Dockerfile is the base image to use to initialize the container. For this tutorial, we will use Ubuntu’s latest LTS:\n",
    "\n",
    "# Use Ubuntu's current LTS\n",
    "FROM ubuntu:jammy-20230804\n",
    "\n",
    "Since we are working to create a Python web service that expose the predictions of a ML model, the next step is to add they key pieces required for the Python service. Let’s make sure that you only include what is necessary to minimize the size, and complexity, of the container as much as possible:\n",
    "\n",
    "# Make sure to not install recommends and to clean the \n",
    "# install to minimize the size of the container as much as possible.\n",
    "RUN apt-get update && \\\n",
    "    apt-get install --no-install-recommends -y python3=3.10.6-1~22.04 && \\\n",
    "    apt-get install --no-install-recommends -y python3-pip=22.0.2+dfsg-1ubuntu0.3 && \\\n",
    "    apt-get install --no-install-recommends -y python3-venv=3.10.6-1~22.04 && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "This instruct Docker to install Python3, pip and venv. It also ensures that apt get cleaned of cached files, that nothing more is installed and that we define the exact version of the package we want to install. That is to ensure that we minimize the size of the container, while making sure that the container can easily be reproduced, with the exact same codebase, any time in the future.\n",
    "\n",
    "Another thing to note: we run multiple commands with a single RUN instruction by piping them together with &&. This is to minimize the number of layers created by Docker for the container, and this is a best practice to follow when creating containers. If you don’t do this and run hadolint, then you will get warning suggesting you to refactor your Dockerfile accordingly.\n",
    "\n",
    "Copy required files\n",
    "Now that the base operating system is installed, the next step is to install all the requirements of the Python project we want to deploy in the container:\n",
    "\n",
    "# Set the working directory within the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy necessary files to the container\n",
    "COPY requirements.txt .\n",
    "COPY main.py .\n",
    "COPY download_models.py .\n",
    "\n",
    "First we define the working directory with the WORKDIR instruction. From now on, every other instruction will run from that directory in the container. We copy the local files: requirements.txt, main.py and download_models.py to the working directory.\n",
    "\n",
    "Create virtual environment\n",
    "Before doing anything with those files, we are better creating a virtual environment where to install all those dependencies. Some people may wonder why we create an environment within an environment? It is further isolation between the container and the Python application to make sure that there is no possibility of dependencies clashes. This is a good best practice to adopt.\n",
    "\n",
    "# Create a virtual environment in the container\n",
    "RUN python3 -m venv .venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "ENV PATH=\"/app/.venv/bin:$PATH\"\n",
    "\n",
    "Install application requirements\n",
    "Once the virtual environment is created and activated in the container, the next step is to install all the required dependencies in that new environment:\n",
    "\n",
    "    # Install Python dependencies from the requirements file\n",
    "RUN pip install --no-cache-dir -r requirements.txt && \\\n",
    "    # Get the models from Hugging Face to bake into the container\n",
    "    python3 download_models.py\n",
    "\n",
    "It runs pip install to install all the dependencies listed in requirements.txt. The dependencies are:\n",
    "\n",
    "transformers==4.30.2\n",
    "flask==2.3.3\n",
    "torch==2.0.1\n",
    "sacremoses==0.0.53\n",
    "sentencepiece==0.1.99\n",
    "\n",
    "Just like the Ubuntu package version, we should (have to!) pin (specify) the exact version of each dependency. This is the best way to ensure that we can reproduce this environment any time in the future and to prevent unexpected crashes because code changed in some downstream dependencies that causes issues with the code.\n",
    "\n",
    "Downloading all models in the container\n",
    "As you can see in the previous RUN command, the next step is to download all models and tokenizers in the working directory such that we bake the model’s artifacts directly in the container. That will ensures that we minimize the time it takes to initialize a container. We spend the time to download all those artifacts at build time instead of run time. The downside is that the containers will be much bigger depending on the models that are required.\n",
    "\n",
    "The download_models.py file is a utility file used to download the Hugging Face models used by the service directly into the container. The code simply download the models and tokenizer files from Hugging Face and save them locally (in the working directory of the container):\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "\n",
    "def download_model(model_path, model_name):\n",
    "    \"\"\"Download a Hugging Face model and tokenizer to the specified directory\"\"\"\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(model_path):\n",
    "        # Create the directory\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    # Save the model and tokenizer to the specified directory\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# For this demo, download the English-French and French-English models\n",
    "download_model('models/en_fr/', 'Helsinki-NLP/opus-mt-en-fr')\n",
    "download_model('models/fr_en/', 'Helsinki-NLP/opus-mt-fr-en')\n",
    "\n",
    "Creating the Flask translation web service endpoint\n",
    "The last thing we have to do with the Dockerfile is to expose the port where the web service will be available and to tell the container what to run when it starts:\n",
    "\n",
    "# Make port 6000 available to the world outside this container\n",
    "EXPOSE 6000\n",
    "\n",
    "ENTRYPOINT [ \"python3\" ]\n",
    "\n",
    "# Run main.py when the container launches\n",
    "CMD [ \"main.py\" ]\n",
    "\n",
    "We expose the port 6000 to the outside world, and we tell Docker to run the python3 command with main.py. The main.py file is a very simple file that register the web service’s path using Flask, and that makes the predictions (translations in this case):\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def get_model(model_path):\n",
    "    \"\"\"Load a Hugging Face model and tokenizer from the specified directory\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the models and tokenizers for each supported language\n",
    "en_fr_model, en_fr_tokenizer = get_model('models/en_fr/')\n",
    "fr_en_model, fr_en_tokenizer = get_model('models/fr_en/')\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def is_translation_supported(from_lang, to_lang):\n",
    "    \"\"\"Check if the specified translation is supported\"\"\"\n",
    "    supported_translations = ['en_fr', 'fr_en']\n",
    "    return f'{from_lang}_{to_lang}' in supported_translations\n",
    "\n",
    "@app.route('/translate/<from_lang>/<to_lang>/', methods=['POST'])\n",
    "def translate_endpoint(from_lang, to_lang):\n",
    "    \"\"\"Translate text from one language to another. This function is \n",
    "    called when a POST request is sent to /translate/<from_lang>/<to_lang>/\"\"\"\n",
    "    if not is_translation_supported(from_lang, to_lang):\n",
    "        return jsonify({'error': 'Translation not supported'}), 400\n",
    "\n",
    "    data = request.get_json()\n",
    "    from_text = data.get(f'{from_lang}_text', '')\n",
    "\n",
    "    if from_text:\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "\n",
    "        match from_lang:\n",
    "            case 'en':        \n",
    "                model = en_fr_model\n",
    "                tokenizer = en_fr_tokenizer\n",
    "            case 'fr':\n",
    "                model = fr_en_model\n",
    "                tokenizer = fr_en_tokenizer\n",
    "\n",
    "        to_text = tokenizer.decode(model.generate(tokenizer.encode(from_text, return_tensors='pt')).squeeze(), skip_special_tokens=True)\n",
    "\n",
    "        return jsonify({f'{to_lang}_text': to_text})\n",
    "    else:\n",
    "        return jsonify({'error': 'Text to translate not provided'}), 400\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=6000, debug=True)\n",
    "\n",
    "Building the container\n",
    "Now that the Dockerfile is completed, the next step is to use it to have Docker to build the actual image of the container. This is done using this command in the terminal:\n",
    "\n",
    "docker build -t localbuild:en_fr_translation_service .\n",
    "\n",
    "Note that we specified a tag to make it easier to manage it in between all the other images that may exists in the environment. The output of the terminal will show every step defined in the Dockerfile, and the processing for each of those step. The final output looks like:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=3122, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "tokenizer = Anthropic().get_tokenizer()\n",
    "tokenizer.encode(str(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = context.split('.')\n",
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runpod\n",
    "\n",
    "runpod.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "endpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n",
    "\n",
    "run_request = endpoint.run(\n",
    "    {\"input\": {\n",
    "        \"context\": context,\n",
    "        \"instruction\": \"You are a question answering bot who uses the provided context to answer a question\\n\",\n",
    "        \"question\": \"What's the purpose of the tutorial?\",\n",
    "        \"target_tokens\": 150,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_PROGRESS\n",
      "{'compressed_prompt': 'You are a question answering bot who uses the provided context to answer a question\\n\\n\\nIn this short will explore how Face be deployed in a Docker Container and a service.\\nThe service it exposes is a service French and.\\nWhy would to do? than to learn about those specific, it is a very to try and test the of models that exists Face, a and environment that can be replicated, shared or deployed elsewhere than on local\\n, to to container with the necessary code and to load and to as webpoints\\nAll code and configurations used to this available in this GitHubository. You simply and the commands this tutorial to the service\\n Docker\\nThe to Docker. The is by simply installing Docker Desktop available on Mac\\n\\nCreating\\n a new repository where will create. is where all instructions written that to\\nI would also strongly to install use hadolint, a really good Docker linter that helps to follow. a for Code if is use as you development\\n image and key\\n first thing you in image use to. use Ubuntus latestTS\\n Use Ubuntu\\'sTS:my-20230\\n are working to a Python service that expose the a model, the to add they key pieces required for the service. Lets make that you only include necessary to size, and complexity the container possible\\n# sure to not install recommends and to clean the# install to container\\n && install-endsy3..6-1~22.04 &&ip=..2+dfsg-1.3-=get     rflib//lists/*\\n\\nThis instruct to install Python3, and. also that apt get cleaned of cached files, that nothing installed that we the the. is to we the container, while the container can be reproduced, same code, time\\nAnother note: we run commands with single RUN instruction by together with is to of layers created Docker and is a follow creating. you dont this run, will get warning suggesting to yourfile\\nCopy\\n the base, to install the requirements the Python project deploy container# the directory\\n CopyY\\nY download_\\n we the directory. From, every will run directory. the local files the\\nCreate\\nBefore anything with we are better environment where install all. Some people may we create environment environment? is further isolation the the Python application to that no of dependencies. is good practice to environment\\nv\\n#\\nV PATH=\"/app\\nInstall application\\n created activated the dependencies in that environment:\\n\\n Install dependencies from\\ntxt &&# the from Face to    3_\\nIt runs. The are\\ners..\\nfl.\\n==\\nsac..\\n..\\nJust the Ubuntu package we should (have to!) pin ( the each dependency. is the we reproduce environment time future and to prevent unexpected crashes because code in some dependencies that causes the\\ning all models the\\n previous the models and the directory we models artifacts. That will we to initialize a. We all artifacts at of. The the bigger areThe download utility file the models service The code simply them locally container transformers import\\n aizer#    # this theFrenchfr/ \\'\\n\\n the Flask serviceThe do to the to# port\\n RunWe that services path that the predictions this case token for each\\n_def\"\"\" the specifiedreturn{ is    \\',if        match_\\':        return({.0\\', port=6000, debug=True)\\n\\nBuilding the container\\nNow that the Dockerfile is completed, the next step is to use it to have Docker to build the actual image of the container. This is done using this command in the terminal:\\n\\ndocker build -t localbuild:en_fr_translation_service.\\n\\nNote that we specified a tag to make it easier to manage it in between all the other images that may exists in the environment. The output of the terminal will show every step defined in the Dockerfile, and the processing for each of those step. The final output looks like:\\n\\nWhat\\'s the purpose of the tutorial?', 'compressed_tokens': 788, 'origin_tokens': 2171, 'ratio': '2.8x', 'saving': ', Saving $0.1 in GPT-4.'}\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "print(run_request.status())\n",
    "response = run_request.output(timeout=60)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compressed_prompt': 'You are a question answering bot who uses the provided '\n",
      "                      'context to answer a question\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      'In this short will explore how Face be deployed in a '\n",
      "                      'Docker Container and a service.\\n'\n",
      "                      'The service it exposes is a service French and.\\n'\n",
      "                      'Why would to do? than to learn about those specific, it '\n",
      "                      'is a very to try and test the of models that exists '\n",
      "                      'Face, a and environment that can be replicated, shared '\n",
      "                      'or deployed elsewhere than on local\\n'\n",
      "                      ', to to container with the necessary code and to load '\n",
      "                      'and to as webpoints\\n'\n",
      "                      'All code and configurations used to this available in '\n",
      "                      'this GitHubository. You simply and the commands this '\n",
      "                      'tutorial to the service\\n'\n",
      "                      ' Docker\\n'\n",
      "                      'The to Docker. The is by simply installing Docker '\n",
      "                      'Desktop available on Mac\\n'\n",
      "                      '\\n'\n",
      "                      'Creating\\n'\n",
      "                      ' a new repository where will create. is where all '\n",
      "                      'instructions written that to\\n'\n",
      "                      'I would also strongly to install use hadolint, a really '\n",
      "                      'good Docker linter that helps to follow. a for Code if '\n",
      "                      'is use as you development\\n'\n",
      "                      ' image and key\\n'\n",
      "                      ' first thing you in image use to. use Ubuntus latestTS\\n'\n",
      "                      \" Use Ubuntu'sTS:my-20230\\n\"\n",
      "                      ' are working to a Python service that expose the a '\n",
      "                      'model, the to add they key pieces required for the '\n",
      "                      'service. Lets make that you only include necessary to '\n",
      "                      'size, and complexity the container possible\\n'\n",
      "                      '# sure to not install recommends and to clean the# '\n",
      "                      'install to container\\n'\n",
      "                      ' && install-endsy3..6-1~22.04 '\n",
      "                      '&&ip=..2+dfsg-1.3-=get     rflib//lists/*\\n'\n",
      "                      '\\n'\n",
      "                      'This instruct to install Python3, and. also that apt '\n",
      "                      'get cleaned of cached files, that nothing installed '\n",
      "                      'that we the the. is to we the container, while the '\n",
      "                      'container can be reproduced, same code, time\\n'\n",
      "                      'Another note: we run commands with single RUN '\n",
      "                      'instruction by together with is to of layers created '\n",
      "                      'Docker and is a follow creating. you dont this run, '\n",
      "                      'will get warning suggesting to yourfile\\n'\n",
      "                      'Copy\\n'\n",
      "                      ' the base, to install the requirements the Python '\n",
      "                      'project deploy container# the directory\\n'\n",
      "                      ' CopyY\\n'\n",
      "                      'Y download_\\n'\n",
      "                      ' we the directory. From, every will run directory. the '\n",
      "                      'local files the\\n'\n",
      "                      'Create\\n'\n",
      "                      'Before anything with we are better environment where '\n",
      "                      'install all. Some people may we create environment '\n",
      "                      'environment? is further isolation the the Python '\n",
      "                      'application to that no of dependencies. is good '\n",
      "                      'practice to environment\\n'\n",
      "                      'v\\n'\n",
      "                      '#\\n'\n",
      "                      'V PATH=\"/app\\n'\n",
      "                      'Install application\\n'\n",
      "                      ' created activated the dependencies in that '\n",
      "                      'environment:\\n'\n",
      "                      '\\n'\n",
      "                      ' Install dependencies from\\n'\n",
      "                      'txt &&# the from Face to    3_\\n'\n",
      "                      'It runs. The are\\n'\n",
      "                      'ers..\\n'\n",
      "                      'fl.\\n'\n",
      "                      '==\\n'\n",
      "                      'sac..\\n'\n",
      "                      '..\\n'\n",
      "                      'Just the Ubuntu package we should (have to!) pin ( the '\n",
      "                      'each dependency. is the we reproduce environment time '\n",
      "                      'future and to prevent unexpected crashes because code '\n",
      "                      'in some dependencies that causes the\\n'\n",
      "                      'ing all models the\\n'\n",
      "                      ' previous the models and the directory we models '\n",
      "                      'artifacts. That will we to initialize a. We all '\n",
      "                      'artifacts at of. The the bigger areThe download utility '\n",
      "                      'file the models service The code simply them locally '\n",
      "                      'container transformers import\\n'\n",
      "                      \" aizer#    # this theFrenchfr/ '\\n\"\n",
      "                      '\\n'\n",
      "                      ' the Flask serviceThe do to the to# port\\n'\n",
      "                      ' RunWe that services path that the predictions this '\n",
      "                      'case token for each\\n'\n",
      "                      '_def\"\"\" the specifiedreturn{ is    \\',if        '\n",
      "                      \"match_':        return({.0', port=6000, debug=True)\\n\"\n",
      "                      '\\n'\n",
      "                      'Building the container\\n'\n",
      "                      'Now that the Dockerfile is completed, the next step is '\n",
      "                      'to use it to have Docker to build the actual image of '\n",
      "                      'the container. This is done using this command in the '\n",
      "                      'terminal:\\n'\n",
      "                      '\\n'\n",
      "                      'docker build -t localbuild:en_fr_translation_service.\\n'\n",
      "                      '\\n'\n",
      "                      'Note that we specified a tag to make it easier to '\n",
      "                      'manage it in between all the other images that may '\n",
      "                      'exists in the environment. The output of the terminal '\n",
      "                      'will show every step defined in the Dockerfile, and the '\n",
      "                      'processing for each of those step. The final output '\n",
      "                      'looks like:\\n'\n",
      "                      '\\n'\n",
      "                      \"What's the purpose of the tutorial?\",\n",
      " 'compressed_tokens': 788,\n",
      " 'origin_tokens': 2171,\n",
      " 'ratio': '2.8x',\n",
      " 'saving': ', Saving $0.1 in GPT-4.'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
